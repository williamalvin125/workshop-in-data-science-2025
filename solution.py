#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Predictive Maintenance – Long-horizon Minute-level Forecasting
==============================================================

This script trains per-sensor Dynamic Harmonic Regression (Fourier terms for daily+weekly)
with ARIMA errors (a.k.a. Fourier+ARIMA / DHR), and optionally learns a lightweight
LightGBM residual corrector. It produces point forecasts for:

  Horizon: 2022-11-11 00:00  →  2023-01-01 00:00  (inclusive)

Files expected (adjust paths below as needed):
  - Failure Training.csv         (Jan 1, 2022 → Oct 31, 2022; minute data)
  - Failure Testing New.csv      (Nov 1, 2022 → Nov 10, 2022; minute data)

Output:
  - submission.csv  with columns: Timestamp, Temperature, Vibration

Core approach:
  1) Fit Fourier-ARIMA per sensor on history up to 2022-11-10 23:59.
  2) Forecast the full horizon using deterministic Fourier regressors.
  3) (Optional) Train a LightGBM regressor on in-sample residuals and add its
     predicted residuals to the DHR forecast (small bias correction).
  4) Clip predictions to robust ranges from training data.

Notes:
  - We intentionally avoid using the `Activity` column at forecast time unless
    you can synthesize it for the future. Fourier regressors already capture
    most periodic structure (daily/weekly).

Dependencies (Kaggle notebook typically has them):
  pandas, numpy, statsmodels, lightgbm (optional)

Author: (generated by ChatGPT)
"""
# fmt: on

import os
import warnings
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple

import numpy as np
import pandas as pd
import json
# Statsmodels
from statsmodels.tsa.statespace.sarimax import SARIMAX

# LightGBM is optional; we guard import.
try:
    from lightgbm import LGBMRegressor
    _HAS_LGBM = True
except Exception:
    _HAS_LGBM = False

# ------------------------------
# Configuration
# ------------------------------

@dataclass
class Config:
    # File paths (auto-detect /mnt/data as default; change for Kaggle)
    train_path: str = "Failure Training.csv"
    holdout_path: str = "Failure Testing New.csv"
    submission_path: str = "submission.csv"

    # Use LightGBM residual booster? (requires lightgbm)
    use_residual_booster: bool = True

    # Hyperparameter tuning flags (to keep runtime reasonable by default)
    do_tune_fourier: bool = True
    do_tune_arima_order: bool = True

    # Default / initial hyperparameters
    daily_K_default: int = 10
    weekly_K_default: int = 6
    arima_order_default: Tuple[int, int, int] = (1, 0, 1)

    # Tuning grids (kept small to be Kaggle-friendly)
    daily_K_grid: Tuple[int, ...] = (8, 10, 12)
    weekly_K_grid: Tuple[int, ...] = (4, 6)
    arima_order_grid: Tuple[Tuple[int, int, int], ...] = ((1, 0, 1), (2, 0, 1), (1, 1, 1))

    # Lags and rolling windows for residual booster
    lags_short: Tuple[int, ...] = (1, 2, 5, 10, 30, 60, 120)
    lags_long: Tuple[int, ...] = (1440, 2880, 10080)  # 1d, 2d, 7d
    roll_windows: Tuple[int, ...] = (15, 60, 240, 1440)

    # LightGBM params (kept small)
    lgbm_params: Dict = None

    # Random seed
    seed: int = 42


def make_default_config() -> Config:
    cfg = Config()
    if cfg.lgbm_params is None:
        cfg.lgbm_params = dict(
            objective="regression",
            n_estimators=1500,
            learning_rate=0.05,
            num_leaves=63,
            feature_fraction=0.9,
            bagging_fraction=0.8,
            bagging_freq=1,
            min_data_in_leaf=100,
            reg_lambda=0.0,
            random_state=cfg.seed,
            n_jobs=-1,
        )
    # If LightGBM is not available, silently disable booster.
    if not _HAS_LGBM:
        cfg.use_residual_booster = False
    return cfg


# ------------------------------
# Utility functions
# ------------------------------

def find_column(df: pd.DataFrame, candidates: List[str]) -> Optional[str]:
    cols_lower = {c.lower(): c for c in df.columns}
    for cand in candidates:
        if cand.lower() in cols_lower:
            return cols_lower[cand.lower()]
    return None


def normalize_columns(df: pd.DataFrame) -> Tuple[pd.DataFrame, str, str, Optional[str]]:
    """
    Attempt to find and standardize the expected columns:
      - Timestamp (datetime)
      - Temperature (float)
      - Vibration (float)
      - Activity (optional)
    """
    ts_col = find_column(df, ["timestamp", "time", "datetime", "date"])
    temp_col = find_column(df, ["temperature", "temp", "temperature_k"])
    vib_col = find_column(df, ["vibration", "rpm", "vibration_rpm"])
    act_col = find_column(df, ["activity", "status", "mode"])

    if ts_col is None or temp_col is None or vib_col is None:
        raise ValueError(
            f"Expected timestamp/temperature/vibration columns, found: {df.columns.tolist()}"
        )

    df = df.rename(
        columns={ts_col: "Timestamp", temp_col: "Temperature", vib_col: "Vibration"}
    )
    if act_col is not None:
        df = df.rename(columns={act_col: "Activity"})

    # Parse datetime
    df["Timestamp"] = pd.to_datetime(df["Timestamp"], errors="coerce", utc=False)
    df = df.dropna(subset=["Timestamp"])
    df = df.sort_values("Timestamp").drop_duplicates(subset=["Timestamp"], keep="last")
    df = df.set_index("Timestamp")
    return df, "Temperature", "Vibration", ("Activity" if "Activity" in df.columns else None)


def ensure_minutely_contiguous(df: pd.DataFrame, cols_to_interpolate: List[str]) -> pd.DataFrame:
    start = df.index.min()
    end = df.index.max()
    full_idx = pd.date_range(start=start, end=end, freq="T")
    df_full = df.reindex(full_idx)

    def clean_numeric_series(s: pd.Series) -> pd.Series:
        s2 = s.astype(str).str.strip()
        s2 = s2.replace(["Bad Input","bad input","BAD INPUT","NA","N/A","#N/A","null","NULL","--","-","", "n/a"], np.nan)
        s2 = s2.str.replace(",", "", regex=False)                       # remove thousands separators
        s2 = s2.str.replace(r"[^0-9\-\+\.eE]", "", regex=True)          # strip units/extra text
        out = pd.to_numeric(s2, errors="coerce").replace([np.inf, -np.inf], np.nan)
        return out

    for c in cols_to_interpolate:
        if c in df_full.columns:
            col = clean_numeric_series(df_full[c])
            df_full[c] = col.interpolate(method="time", limit_direction="both")

    num_cols = df_full.select_dtypes(include=[np.number]).columns.tolist()
    df_full[num_cols] = df_full[num_cols].ffill().bfill()
    return df_full


def make_fourier(index: pd.DatetimeIndex, daily_K: int, weekly_K: int) -> np.ndarray:
    """
    Build deterministic Fourier regressors anchored to time-of-day (daily) and
    time-of-week (weekly). This keeps phase consistent across train/forecast ranges.
    """
    # minute of day: 0..1439
    mod = index.hour * 60 + index.minute
    # day of week (Mon=0..Sun=6); minute of week: 0..10079
    dow = index.dayofweek
    mow = dow * 1440 + mod

    X_list = []
    # Daily harmonics
    for k in range(1, daily_K + 1):
        ang_d = 2.0 * np.pi * k * (mod / 1440.0)
        X_list.append(np.sin(ang_d))
        X_list.append(np.cos(ang_d))
    # Weekly harmonics
    for k in range(1, weekly_K + 1):
        ang_w = 2.0 * np.pi * k * (mow / 10080.0)
        X_list.append(np.sin(ang_w))
        X_list.append(np.cos(ang_w))

    X = np.column_stack(X_list).astype(np.float64)
    return X


def mse(y_true: np.ndarray, y_pred: np.ndarray) -> float:
    return float(np.mean((np.asarray(y_true, dtype=float) - np.asarray(y_pred, dtype=float)) ** 2))


def seasonal_naive(y_hist: pd.Series, target_index: pd.DatetimeIndex, season_minutes: int = 10080) -> pd.Series:
    """
    Seasonal naive: y_hat(t) = y(t - season_minutes). Produces a prediction for
    each timestamp in target_index, using the history series y_hist (indexed by timestamp).
    """
    delta = pd.Timedelta(minutes=season_minutes)
    src_index = target_index - delta
    # Align values by timestamp
    preds = y_hist.reindex(src_index).values
    return pd.Series(preds, index=target_index)


def robust_clip(arr: np.ndarray, series_hist: pd.Series) -> np.ndarray:
    """
    Clip predictions using robust training-range bounds to avoid catastrophic outliers.
    """
    q1, q3 = np.quantile(series_hist.values.astype(float), [0.25, 0.75])
    iqr = q3 - q1
    if iqr <= 0:
        # Fallback to std-based bounds
        mu = float(np.mean(series_hist.values))
        sd = float(np.std(series_hist.values))
        lo = np.min(series_hist.values) - 3 * (sd if sd > 0 else 1.0)
        hi = np.max(series_hist.values) + 3 * (sd if sd > 0 else 1.0)
    else:
        lo = q1 - 3.0 * iqr
        hi = q3 + 3.0 * iqr
    return np.clip(arr, lo, hi)


# ------------------------------
# Modeling helpers
# ------------------------------

@dataclass
class DHRResult:
    model: SARIMAX
    res: any  # fitted results
    order: Tuple[int, int, int]
    daily_K: int
    weekly_K: int
    fitted_in_sample: pd.Series  # aligned to hist index
    val_pred: Optional[pd.Series]  # predictions on validation (if tuned), else None


def fit_fourier_arima(
    y_train: pd.Series,
    X_train: np.ndarray,
    order: Tuple[int, int, int] = (1, 0, 1),
) -> any:
    """
    Fit a SARIMAX with exogenous Fourier regressors; return fitted results object.
    """
    mod = SARIMAX(
        endog=y_train.astype(float).values,
        exog=X_train,
        order=order,
        seasonal_order=(0, 0, 0, 0),
        trend="c",
        enforce_stationarity=False,
        enforce_invertibility=False,
        concentrate_scale=True,
        # measurement_error=True  # (optional) can stabilize some fits
    )
    with warnings.catch_warnings():
        warnings.simplefilter("ignore")
        res = mod.fit(disp=False, method="lbfgs", maxiter=200)
    return res


def tune_dhr(
    series_name: str,
    y_hist: pd.Series,
    train_end: pd.Timestamp,
    val_start: pd.Timestamp,
    val_end: pd.Timestamp,
    cfg: Config,
) -> DHRResult:
    """
    Tune daily/weekly Fourier K and ARIMA order on a single hold-out window (Nov 1-10).
    Returns a DHRResult with best params and in-sample fitted values over history
    using the *final refit* (train+val).
    """
    print(f"\n[{series_name}] Tuning DHR hyperparameters...")
    # Split
    y_train = y_hist.loc[: train_end]
    y_val = y_hist.loc[val_start: val_end]

    best = None
    best_mse = np.inf
    best_cfg = (cfg.daily_K_default, cfg.weekly_K_default, cfg.arima_order_default)

    # Step 1: tune Fourier K with default ARIMA order
    daily_candidates = cfg.daily_K_grid if cfg.do_tune_fourier else (cfg.daily_K_default,)
    weekly_candidates = cfg.weekly_K_grid if cfg.do_tune_fourier else (cfg.weekly_K_default,)

    for kd in daily_candidates:
        for kw in weekly_candidates:
            try:
                X_tr = make_fourier(y_train.index, kd, kw)
                X_va = make_fourier(y_val.index, kd, kw)
                res = fit_fourier_arima(y_train, X_tr, order=cfg.arima_order_default)
                pred = res.get_forecast(steps=len(y_val), exog=X_va).predicted_mean
                cur_mse = mse(y_val.values, pred)
                print(f"  Kd={kd:2d}, Kw={kw:2d}, order={cfg.arima_order_default} → val MSE={cur_mse:,.4f}")
                if cur_mse < best_mse:
                    best_mse = cur_mse
                    best_cfg = (kd, kw, cfg.arima_order_default)
            except Exception as e:
                print(f"   (skip) kd={kd}, kw={kw}: {e}")
                continue
    
    
    # Step 2: tune ARIMA order with selected K values
    order_candidates = cfg.arima_order_grid if cfg.do_tune_arima_order else (cfg.arima_order_default,)
    for ord_ in order_candidates:
        try:
            X_tr = make_fourier(y_train.index, kd_best, kw_best)
            X_va = make_fourier(y_val.index, kd_best, kw_best)
            res = fit_fourier_arima(y_train, X_tr, order=ord_)
            pred = res.get_forecast(steps=len(y_val), exog=X_va).predicted_mean
            cur_mse = mse(y_val.values, pred)
            print(f"  Confirm order={ord_} with Kd={kd_best}, Kw={kw_best} → val MSE={cur_mse:,.4f}")
            if cur_mse < best_mse:
                best_mse = cur_mse
                kd_best, kw_best, order_best = kd_best, kw_best, ord_
        except Exception as e:
            print(f"   (skip order) {ord_}: {e}")
            continue

    kd_best, kw_best, order_best = best_cfg
    # Final refit on train+val (full history up to val_end) with best params
    hist_index = y_hist.loc[: val_end].index
    y_hist_fit = y_hist.loc[hist_index]
    X_hist = make_fourier(hist_index, kd_best, kw_best)
    res_final = fit_fourier_arima(y_hist_fit, X_hist, order=order_best)
    fitted = pd.Series(res_final.fittedvalues, index=hist_index)

    # Report best

    print(f"[{series_name}] Selected Kd={kd_best}, Kw={kw_best}, order={order_best}; best val MSE={best_mse:,.4f}")

    # Also keep the validation prediction from tuning (recompute with best)
    X_tr = make_fourier(y_train.index, kd_best, kw_best)
    X_va = make_fourier(y_val.index, kd_best, kw_best)
    res_for_val = fit_fourier_arima(y_train, X_tr, order=order_best)
    val_pred = pd.Series(
        res_for_val.get_forecast(steps=len(y_val), exog=X_va).predicted_mean, index=y_val.index
    )

    return DHRResult(
        model=None,
        res=res_final,
        order=order_best,
        daily_K=kd_best,
        weekly_K=kw_best,
        fitted_in_sample=fitted,
        val_pred=val_pred,
    )


def build_residual_features(
    full_index: pd.DatetimeIndex,
    y_full: pd.Series,
    dhr_full: pd.Series,
    other_full: Optional[pd.Series],
    cfg: Config,
) -> pd.DataFrame:
    """
    Build residual-correction features over the *full_index* timeline using the
    concatenated series (history + DHR-forecast for future). We only *train* on
    the history slice later; the rest is used for forecasting.
    """
    df = pd.DataFrame(index=full_index)
    # Calendar
    df["min_of_day"] = df.index.hour * 60 + df.index.minute
    df["dow"] = df.index.dayofweek
    df["is_weekend"] = (df["dow"] >= 5).astype(int)
    df["min_of_week"] = df["dow"] * 1440 + df["min_of_day"]

    # Target series lags
    def add_lags(base: pd.Series, prefix: str, lags: Tuple[int, ...]):
        for L in lags:
            df[f"{prefix}_lag{L}"] = base.shift(L)

    add_lags(y_full, "y", cfg.lags_short + cfg.lags_long)

    # Rolling stats on y (use past-only by shifting 1)
    y_s = y_full.shift(1)
    for w in cfg.roll_windows:
        df[f"y_roll{w}_mean"] = y_s.rolling(w).mean()
        df[f"y_roll{w}_std"] = y_s.rolling(w).std()

    # DHR prediction as a feature (helps learn bias)
    df["dhr_pred"] = dhr_full

    # Optional cross-sensor lag features (a couple of lags to stay light)
    if other_full is not None:
        add_lags(other_full, "oy", (60, 1440))  # 1h and 1d lag of other sensor

    # Final type-cast
    for c in df.columns:
        df[c] = pd.to_numeric(df[c], errors="coerce")
    return df


def train_residual_model(
    X_hist: pd.DataFrame, y_hist: pd.Series, dhr_hist: pd.Series, cfg: Config
):
    """
    Train LightGBM residual regressor: target = y_hist - dhr_hist
    """
    target = (y_hist - dhr_hist).loc[X_hist.index]
    mask = np.isfinite(X_hist.values).all(axis=1) & np.isfinite(target.values)
    X_tr = X_hist.loc[mask].astype(np.float32)
    y_tr = target.loc[mask].astype(np.float32)

    if X_tr.empty:
        raise RuntimeError("No valid rows to train residual model after lag/rolling drops.")

    mdl = LGBMRegressor(**cfg.lgbm_params)
    mdl.fit(X_tr, y_tr)
    return mdl


# ------------------------------
# Main pipeline
# ------------------------------

def main():
    cfg = make_default_config()
    print("Config:", cfg)

    # Load training and hold-out (Nov 1-10) data
    if not os.path.exists(cfg.train_path):
        raise FileNotFoundError(f"Train file not found at: {cfg.train_path}")
    if not os.path.exists(cfg.holdout_path):
        raise FileNotFoundError(f"Hold-out file not found at: {cfg.holdout_path}")

    df_tr_raw = pd.read_csv(cfg.train_path)
    df_ho_raw = pd.read_csv(cfg.holdout_path)

    df_tr, temp_col, vib_col, act_col = normalize_columns(df_tr_raw)
    df_ho, _, _, _ = normalize_columns(df_ho_raw)

    # Enforce 1-min frequency and fill tiny gaps with time-interp for sensors
    df_tr = ensure_minutely_contiguous(df_tr, [temp_col, vib_col])
    df_ho = ensure_minutely_contiguous(df_ho, [temp_col, vib_col])

    # Combine hist (up to Nov 10 inclusive)
    df_hist = pd.concat([df_tr, df_ho]).sort_index()
    df_hist = df_hist[~df_hist.index.duplicated(keep="last")]

    # Determine validation window from holdout file
    val_start = df_ho.index.min()
    val_end = df_ho.index.max()
    train_end = val_start - pd.Timedelta(minutes=1)

    print(f"History range: {df_hist.index.min()} → {df_hist.index.max()}")
    print(f"Validation window: {val_start} → {val_end} (len={len(df_ho)})")

    # Forecast horizon: 2022-11-11 00:00 → 2023-01-01 00:00 (inclusive)
    horizon_start = pd.Timestamp("2022-11-11 00:00")
    horizon_end = pd.Timestamp("2023-01-01 00:00")
    forecast_index = pd.date_range(start=horizon_start, end=horizon_end, freq="T")

    # Prepare per-sensor series
    sensors = {"Temperature": df_hist[temp_col].astype(float), "Vibration": df_hist[vib_col].astype(float)}

    # Evaluate a simple seasonal naive baseline on the validation window
    print("\nSeasonal naive baseline on validation (minute-of-week):")
    for name, y in sensors.items():
        naive_val = seasonal_naive(y, df_ho.index, season_minutes=10080)
        val_true = y.loc[df_ho.index]
        naive_mse = mse(val_true.values, naive_val.values)
        bias = (val_true.values - naive_val.values)
        bias_correction = float(np.nanmean(bias))
        naive_aligned = naive_val + bias_correction  # alignment using val (for info only)
        naive_aligned_mse = mse(val_true.values, naive_aligned.values)
        print(f"  [{name}] naive MSE = {naive_mse:,.4f}; aligned (info) = {naive_aligned_mse:,.4f}")

    # --- DHR tuning, refit, and forecasting per sensor ---
    dhr_results: Dict[str, DHRResult] = {}
    dhr_forecasts: Dict[str, pd.Series] = {}
    fitted_hist: Dict[str, pd.Series] = {}

    for name, y in sensors.items():
        res = tune_dhr(name, y, train_end, val_start, val_end, cfg)
        dhr_results[name] = res
        fitted_hist[name] = res.fitted_in_sample

        # Forecast using the refit model on hist up to val_end
        X_fore = make_fourier(forecast_index, res.daily_K, res.weekly_K)
        fore = res.res.get_forecast(steps=len(forecast_index), exog=X_fore).predicted_mean
        dhr_forecasts[name] = pd.Series(fore, index=forecast_index)

        # Report validation MSE from DHR (based on tuning run)
        if res.val_pred is not None:
            val_true = y.loc[val_start: val_end]
            dhr_mse_val = mse(val_true.values, res.val_pred.values)
            print(f"[{name}] DHR val MSE = {dhr_mse_val:,.4f}")

    # --- Optional residual booster (LightGBM) ---
    final_forecasts: Dict[str, pd.Series] = {}
    if cfg.use_residual_booster:
        if not _HAS_LGBM:
            print("\nLightGBM not available, skipping residual booster.")
        else:
            print("\nTraining residual boosters (LightGBM)...")
            names = list(sensors.keys())  # ["Temperature","Vibration"]

            # Build full index: history up to val_end + forecast horizon
            hist_index = df_hist.loc[: val_end].index
            full_index = hist_index.append(forecast_index)

            # Prepare other-sensor series for cross-lags
            other_map = {
                "Temperature": sensors["Vibration"],
                "Vibration": sensors["Temperature"],
            }

            for name in names:
                y_hist = sensors[name].loc[hist_index]
                y_dhr_hist = fitted_hist[name].loc[hist_index]

                # Create concatenated series for features (history + DHR forecast for future)
                y_concat = pd.concat([y_hist, dhr_forecasts[name]]).reindex(full_index)
                dhr_concat = pd.concat([y_dhr_hist, dhr_forecasts[name]]).reindex(full_index)

                # Other sensor concat (for cross lags): use its DHR forecast for future
                other_hist = other_map[name].loc[hist_index]
                other_concat = pd.concat([other_hist, dhr_forecasts["Vibration" if name=="Temperature" else "Temperature"]]).reindex(full_index)

                feats_full = build_residual_features(full_index, y_concat, dhr_concat, other_concat, cfg)

                # Training features = restrict to hist_index, dropping NaNs from lags/rollings
                X_hist = feats_full.loc[hist_index].dropna()
                # Align targets (residuals) to X_hist
                y_hist_aligned = y_hist.loc[X_hist.index]
                dhr_hist_aligned = dhr_concat.loc[X_hist.index]

                # Train model
                try:
                    mdl = train_residual_model(X_hist, y_hist_aligned, dhr_hist_aligned, cfg)
                except Exception as e:
                    print(f"  [{name}] Residual model training skipped due to: {e}")
                    final_forecasts[name] = dhr_forecasts[name].copy()
                    continue

                # Forecast residuals on horizon
                X_fore = feats_full.loc[forecast_index].copy()
                # It's safe to forward-fill any initial NaNs if present (should be rare)
                X_fore = X_fore.fillna(method="ffill").fillna(method="bfill")
                resid_pred = mdl.predict(X_fore.astype(np.float32))

                boosted = dhr_forecasts[name].values + resid_pred

                # Clip to robust training-range bounds
                boosted = robust_clip(boosted, y_hist)

                final_forecasts[name] = pd.Series(boosted, index=forecast_index)

                print(f"  [{name}] Residual booster trained; added corrections to DHR forecast.")
    else:
        print("\nResidual booster disabled; using pure DHR forecasts.")
        for name in sensors.keys():
            final_forecasts[name] = dhr_forecasts[name].copy()

    # --- Assemble submission ---
    out_temp = final_forecasts["Temperature"].copy()
    out_vib = final_forecasts["Vibration"].copy()

    # Clip one more time just in case
    out_temp = pd.Series(robust_clip(out_temp.values, sensors["Temperature"].loc[: val_end]), index=out_temp.index)
    out_vib = pd.Series(robust_clip(out_vib.values, sensors["Vibration"].loc[: val_end]), index=out_vib.index)

    submission = pd.DataFrame(
        {"Timestamp": forecast_index, "Temperature": out_temp.values, "Vibration": out_vib.values}
    )
    submission.to_csv(cfg.submission_path, index=False)
    print(f"\nWrote submission to: {cfg.submission_path}")
    print(submission.head())
    print(submission.tail())

    # --- Print a compact JSON with chosen hyperparameters ---
    chosen = {
        name: dict(daily_K=dhr_results[name].daily_K, weekly_K=dhr_results[name].weekly_K, order=dhr_results[name].order)
        for name in dhr_results.keys()
    }
    print("\nChosen hyperparameters per sensor:")
    print(json.dumps(chosen, indent=2))

    print("\nDone.")
    

if __name__ == "__main__":
    warnings.filterwarnings("ignore", category=UserWarning)
    warnings.filterwarnings("ignore", category=FutureWarning)
    main()
